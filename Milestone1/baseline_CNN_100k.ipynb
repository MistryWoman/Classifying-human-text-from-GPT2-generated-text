{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Baseline_CNN_100k.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvEg-06BHycP",
        "colab_type": "code",
        "outputId": "7e9528b8-0132-433c-cb69-f81cd9b114f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fGixuwMDwpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.data import Iterator, BucketIterator\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/data/\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5z9RPU7P4sS",
        "colab_type": "code",
        "outputId": "85f704e2-e0e2-4a3d-b667-e25d4374edd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -U torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us_0vewlDzgd",
        "colab_type": "code",
        "outputId": "d45be561-96d7-4eea-f43b-fab97f81bec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYh0BzRTDwpu",
        "colab_type": "code",
        "outputId": "f3c05586-99d0-4631-c286-1de5a8660a70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "manual_seed = 77\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed(manual_seed)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHMsTWXgDwp5",
        "colab_type": "text"
      },
      "source": [
        "#EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axp-pCd3Dwp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading in 50k data\n",
        "df = pd.read_csv(path+\"subset_100k.csv\", index_col=0, encoding=\"utf-8\").reset_index(drop=True)\n",
        "# df_shuffled = df.sample(frac=1, random_state=123) #shuffle rows randomly\n",
        "# df_shuffled = df_shuffled.drop(columns=\"index\") #drops index to only keep text and label\n",
        "train, validate, test = np.split(df.sample(frac=1, random_state=123).drop(columns=\"index\"), \n",
        "                                                                          [int(.6*len(df)), int(.8*len(df))])\n",
        "train_texts, train_labels = zip(*train.values) #resulting type is tuples\n",
        "valid_texts, valid_labels = zip(*validate.values)\n",
        "test_texts, test_labels = zip(*test.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2APQb4cd6t-z",
        "colab_type": "code",
        "outputId": "8e114533-18af-428f-9295-ba7ff32cb1fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#check distribution of shuffled data by class (random state 123)\n",
        "\n",
        "print(Counter(train_labels))\n",
        "print(Counter(valid_labels))\n",
        "print(Counter(test_labels))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0: 30118, 1: 29882})\n",
            "Counter({1: 10124, 0: 9876})\n",
            "Counter({0: 10006, 1: 9994})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug6_viDP6t4u",
        "colab_type": "code",
        "outputId": "fa2a0bf5-8d5b-49f0-8f62-6c95a647d6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(train[:3])\n",
        "print(train[-3:])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text  labels\n",
            "42083  MUMBAI: India's star professional boxer and Ol...       0\n",
            "71825  P.S: This may not seem like something that is ...       1\n",
            "99535  This pattern is available\\n\\nThe pattern is fr...       1\n",
            "                                                    text  labels\n",
            "20737  Ohio Gov. John Kasich is continuing his attack...       0\n",
            "37688  This page has been flagged for a review of its...       0\n",
            "83502  An Ottawa family is speaking out after a photo...       1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jscE4_hGNba",
        "colab_type": "code",
        "outputId": "31251484-6d11-468d-ee4c-e4967bcdf62d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(validate[:3])\n",
        "print(validate[-3:])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text  labels\n",
            "75082  A recent study has found that children are mor...       1\n",
            "88345  Greetings Friends,\\n\\n\\nI hope everyone is cel...       1\n",
            "95838  The UESPWiki – Your source for The Elder Scrol...       1\n",
            "                                                    text  labels\n",
            "89728  The Washington Post reports this week that the...       1\n",
            "32245  Climate Change Protesters Canceled March in Co...       0\n",
            "4458   When Canadiens general manager Marc Bergevin s...       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxev-MELGNYX",
        "colab_type": "code",
        "outputId": "66c3944b-45a2-400c-e919-888617363efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(test[:3])\n",
        "print(test[-3:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text  labels\n",
            "21563  According to local services' estimates, about ...       0\n",
            "67879  \"I feel extremely lucky that I was at the righ...       1\n",
            "86351  The Department of Homeland Security (DHS) is t...       1\n",
            "                                                    text  labels\n",
            "17730  Just checked into the speaker forum to look fo...       0\n",
            "28030  SHOWCASE ! That Bastard Is Trying To Steal Our...       0\n",
            "15725  Touch Of Malice Check List\\n\\nMausoleum\\n\\n1\\n...       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcIpj5u_GNVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PROuds5qDwqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create train, val, test subsets from 50k subset\n",
        "\n",
        "# train.to_csv(path+\"subset_50k_train.csv\", index=False)\n",
        "# validate.to_csv(path+\"subset_50k_valid.csv\", index=False)\n",
        "# test.to_csv(path+\"subset_50k_test.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WwCrAhh-DwqT",
        "colab_type": "code",
        "outputId": "1472a940-4d15-416c-e2d1-b5bfc0f02c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(len(train))\n",
        "print(len(validate))\n",
        "print(len(test))\n",
        "\n",
        "print(len(train_texts))\n",
        "print(len(valid_texts))\n",
        "print(len(test_texts))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "20000\n",
            "20000\n",
            "60000\n",
            "20000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGVuozlj-3ga",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v0ZfjMb-3MX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HXiqTnUDwqY",
        "colab_type": "code",
        "outputId": "c2827b1f-9c85-4b22-8450-a7245e64dd32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#50k subset\n",
        "\n",
        "n_jobs=None\n",
        "verbose=False\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "vect = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_features=2**21)\n",
        "train_features = vect.fit_transform(train_texts)\n",
        "valid_features = vect.transform(valid_texts)\n",
        "test_features = vect.transform(test_texts)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "params = {'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100]} #changed from original code\n",
        "search = GridSearchCV(model, params, cv=5, n_jobs=n_jobs, verbose=verbose)\n",
        "search.fit(sparse.vstack([train_features, valid_features]), train_labels+valid_labels)\n",
        "print(search.best_params_)\n",
        "model = model.set_params(**search.best_params_)\n",
        "model.fit(train_features, train_labels)\n",
        "valid_accuracy = model.score(valid_features, valid_labels)*100.\n",
        "test_accuracy = model.score(test_features, test_labels)*100.\n",
        "data = {\n",
        "    'valid_accuracy':valid_accuracy,\n",
        "    'test_accuracy':test_accuracy\n",
        "}\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"time in seconds:\", end-start)\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 100}\n",
            "{'valid_accuracy': 85.79, 'test_accuracy': 87.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7niM3RYDwqi",
        "colab_type": "text"
      },
      "source": [
        "# Load and prepare dataset for pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPyePrH3Dwqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spacy_en = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\",\"textcat\"])\n",
        "# spacy_en.add_pipe(spacy_en.create_pipe('sentencizer')) #adding sentence tokenizer\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm') #changed syntax?\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
        "LABEL = Field(sequential=False, unk_token = None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb-Pc1vtDwqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#100k subset\n",
        "train, val, test = TabularDataset.splits(\n",
        "               path=path, # the root directory where the data lies\n",
        "               train='subset_100k_train.csv', validation=\"subset_100k_valid.csv\", test=\"subset_100k_test.csv\", # file names\n",
        "               format='csv',\n",
        "               skip_header=True, \n",
        "               fields=[('text', TEXT), ('label', LABEL)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBZUYNmrDwqu",
        "colab_type": "code",
        "outputId": "ced6c485-44d7-4e1e-9c92-4996316c861e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#100k subset\n",
        "\n",
        "TEXT.build_vocab(train, min_freq=2)\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n",
        "print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size of TEXT: 145549\n",
            "Vocabulary size of LABEL: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "555zXk3JAMyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOZ4YW9KAMvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnPFViAHAMsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45ZqiRdsDwqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, val_iter, test_iter = BucketIterator.splits( #don't we usually not do test_iter?????\n",
        " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
        " batch_sizes=(64,256,256),\n",
        " sort_key=lambda x: len(x.text), \n",
        " sort=True,\n",
        "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
        " sort_within_batch=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWGVVU7dI0UD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLzV6PFZI0Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZNAD9ecDwq5",
        "colab_type": "code",
        "outputId": "023d45e3-d6dd-4ed1-986b-3e0fc9438b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# create a single batch and terminate the loop\n",
        "i = 0\n",
        "for batch in val_iter:\n",
        "    texts = batch.text\n",
        "    labels = batch.label\n",
        "    print(i)\n",
        "    print(texts)\n",
        "    print(labels)\n",
        "    i+=1\n",
        "    if i ==3:\n",
        "      break\n",
        "\n",
        "    #break  #we use first batch as an example.\n",
        "    \n",
        "print('texts:', texts.shape)\n",
        "print('labels:', labels.shape)\n",
        "\n",
        "# texts: [length, batch size]\n",
        "# labels: [batch size]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "tensor([[    2,    11,    14,  ...,     0,    21, 11695],\n",
            "        [  156,    24,    13,  ...,     8,   115,  2534],\n",
            "        [    6,    13,   266,  ..., 17087,   101,  1805],\n",
            "        ...,\n",
            "        [  152,  9754,  2035,  ...,     1,     1,     1],\n",
            "        [ 1035,     4,  1538,  ...,     1,     1,     1],\n",
            "        [    4,    11,     4,  ...,     1,     1,     1]])\n",
            "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
            "        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1])\n",
            "1\n",
            "tensor([[  6552,    748,  27342,  ...,     34,    192,    628],\n",
            "        [    13,      2, 120582,  ...,   4086,    519,    877],\n",
            "        [     7,  21297,   3091,  ...,  34586,     21,   1141],\n",
            "        ...,\n",
            "        [    18,     14,      0,  ...,      1,      1,      1],\n",
            "        [   666,      4,     31,  ...,      1,      1,      1],\n",
            "        [     4,     11,      4,  ...,      1,      1,      1]])\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n",
            "2\n",
            "tensor([[ 309,   19,   14,  ..., 1267,  318, 4703],\n",
            "        [   4,   22,   22,  ...,    9,   24,    0],\n",
            "        [   2, 4802, 1676,  ..., 6390,  211,   22],\n",
            "        ...,\n",
            "        [ 268,   23,  231,  ...,    1,    1,    1],\n",
            "        [  61, 2428, 7557,  ...,    1,    1,    1],\n",
            "        [   4,   35,    4,  ...,    1,    1,    1]])\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
            "texts: torch.Size([66, 256])\n",
            "labels: torch.Size([256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru0oxXl1Dwq-",
        "colab_type": "text"
      },
      "source": [
        "# CNN baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRB5tHL1Dwq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To define a CNN class\n",
        "class CNN_Text(nn.Module):\n",
        "    def __init__(self, vocabulary_size, embedding_dim, output_size, kernel_num, region_sizes, dropout):\n",
        "        '''\n",
        "        vocabulary_size: vocabulary size\n",
        "        embedding_dim: word embedding size\n",
        "        output_size: number of classes in prediction\n",
        "        kernel_num: number of kernels (number of output channels of convolutional layers)\n",
        "        region_sizes: height of kernels of convolutional layers\n",
        "        dropout: dropout rate\n",
        "        '''\n",
        "        super(CNN_Text, self).__init__()\n",
        "        # the size of input channel is 1.\n",
        "        Ci = 1\n",
        "        \n",
        "        # word embedding layer\n",
        "        self.embeddings = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim = embedding_dim )\n",
        "        \n",
        "        # convolution with kernels\n",
        "        self.convolution_layers = nn.ModuleList([nn.Conv2d(in_channels = Ci, out_channels = kernel_num, kernel_size = (K, embedding_dim)) for K in region_sizes])\n",
        "        \n",
        "        # a dropout layer\n",
        "        self.dropout = nn.Dropout(dropout) \n",
        "        \n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * kernel_num, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input x  [sequence length, batch size]\n",
        "        \n",
        "        input_embeddings = self.embeddings(x)  \n",
        "        # (batch size, word_sequence, embedding_dim) word embedding\n",
        "\n",
        "        input_embeddings = input_embeddings.permute(1,0,2)\n",
        "        input_embeddings = input_embeddings.unsqueeze(1)\n",
        "        #  [batch size, number of channel is one, sequence length, embeeding size]\n",
        "\n",
        "        # convolutional layers\n",
        "        convolute_outputs = [F.relu(conv(input_embeddings)).squeeze(3) for conv in self.convolution_layers]  \n",
        "        \n",
        "        # to get the maximum value of filtered tensor\n",
        "        max_pooling_outputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in convolute_outputs] \n",
        "        \n",
        "        concat_list = torch.cat(max_pooling_outputs, 1) # concatenate representations\n",
        "        \n",
        "        drop_output = self.dropout(concat_list)  # add drop layer\n",
        "        \n",
        "        fc1_output = self.fc(drop_output)  # get the fc1 using a fully connected layer\n",
        "        \n",
        "        final_output = F.softmax(fc1_output,dim=1)\n",
        "        \n",
        "        return final_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl95C6aODwrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters\n",
        "\n",
        "# the vocabulary size\n",
        "vocabulary_size = len(TEXT.vocab.stoi) \n",
        "\n",
        "# Dimension of word embedding is 300. Namely, each word is expressed by a vector that has 300 dimensions.\n",
        "embedding_dim = 300 \n",
        "\n",
        "# region size as 2, 3, and 4\n",
        "kernel_sizes = [2,3,4] \n",
        "\n",
        "# the number of kernel in each region size\n",
        "kernels_num = 32  \n",
        "\n",
        "# The dropout rate is set to be 0.5.\n",
        "dropout = 0.5\n",
        "\n",
        "# The output size of labels.\n",
        "output_size = 2\n",
        "\n",
        "# learning rate is set to be 0.01. CHANGED FROM ORIGINAL\n",
        "lr = 0.0001        \n",
        "\n",
        "# The number of iteration is set to be 5.\n",
        "num_epoch = 5  \n",
        "\n",
        "# employ class CNN_Text and assign to cnn\n",
        "model = CNN_Text(vocabulary_size, embedding_dim, output_size, kernels_num, kernel_sizes, dropout).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZGhseSCDwrI",
        "colab_type": "code",
        "outputId": "bae683e5-ad82-4be3-af58-a4a1dc5c87e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_Text(\n",
              "  (embeddings): Embedding(146142, 300)\n",
              "  (convolution_layers): ModuleList(\n",
              "    (0): Conv2d(1, 32, kernel_size=(2, 300), stride=(1, 1))\n",
              "    (1): Conv2d(1, 32, kernel_size=(3, 300), stride=(1, 1))\n",
              "    (2): Conv2d(1, 32, kernel_size=(4, 300), stride=(1, 1))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=96, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3nnVOT7DwrN",
        "colab_type": "code",
        "outputId": "d64ffc69-023a-442e-f5f7-3caed92ec996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 43,929,290 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cccAFOyUDwrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)   # define a optimizer for backpropagation\n",
        "loss_func = nn.CrossEntropyLoss()   # define loss funtion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxVXTzNCDwrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        batch_input, labels = batch.text, batch.label\n",
        "        batch_input = batch_input.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(batch_input)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.cpu().item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            batch_input, labels = batch.text, batch.label\n",
        "            batch_input = batch_input.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_input)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            epoch_loss += loss.cpu().item()\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return epoch_loss / len(iterator), accuracy, f1score\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tnMpUT6D5VN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB38dh_wD5R8",
        "colab_type": "code",
        "outputId": "d04fd722-4533-44b5-d57b-9e725aec8482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(val_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXWX9rwLDwra",
        "colab_type": "code",
        "outputId": "6025b333-5398-4eeb-e7bf-0a3bb9f490a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 50k subset\n",
        "MAX_EPOCH = 20\n",
        "total_step = len(train_iter)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(MAX_EPOCH, desc=\"Epoch\"):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iter, optimizer, loss_func)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(model, val_iter, loss_func)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state_dict_model = model.state_dict() \n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': state_dict_model,\n",
        "        'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "    #torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt_cnn/CNN_TEXT_\"+str(epoch+1)+\".pt\")\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, \n",
        "                                                                MAX_EPOCH, train_loss, val_loss, val_acc, val_f1))\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   5%|▌         | 1/20 [00:35<11:19, 35.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 35s\n",
            "\n",
            " Epoch [1/20], Train Loss: 0.5524, Validation Loss: 0.5830, Validation Accuracy: 0.6923, Validation F1: 0.6713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 2/20 [01:10<10:37, 35.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 0m 34s\n",
            "\n",
            " Epoch [2/20], Train Loss: 0.5374, Validation Loss: 0.5694, Validation Accuracy: 0.7110, Validation F1: 0.6945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  15%|█▌        | 3/20 [01:44<09:57, 35.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 0m 34s\n",
            "\n",
            " Epoch [3/20], Train Loss: 0.5225, Validation Loss: 0.5647, Validation Accuracy: 0.7178, Validation F1: 0.7012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 4/20 [02:20<09:24, 35.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 0m 35s\n",
            "\n",
            " Epoch [4/20], Train Loss: 0.5087, Validation Loss: 0.5510, Validation Accuracy: 0.7349, Validation F1: 0.7217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 5/20 [02:54<08:45, 35.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 0m 34s\n",
            "\n",
            " Epoch [5/20], Train Loss: 0.4914, Validation Loss: 0.5409, Validation Accuracy: 0.7486, Validation F1: 0.7372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 6/20 [03:29<08:08, 34.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 0m 34s\n",
            "\n",
            " Epoch [6/20], Train Loss: 0.4751, Validation Loss: 0.5370, Validation Accuracy: 0.7510, Validation F1: 0.7395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  35%|███▌      | 7/20 [04:03<07:31, 34.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 0m 34s\n",
            "\n",
            " Epoch [7/20], Train Loss: 0.4582, Validation Loss: 0.5264, Validation Accuracy: 0.7665, Validation F1: 0.7573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 8/20 [04:39<07:00, 35.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 0m 35s\n",
            "\n",
            " Epoch [8/20], Train Loss: 0.4420, Validation Loss: 0.5210, Validation Accuracy: 0.7730, Validation F1: 0.7644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  45%|████▌     | 9/20 [05:13<06:23, 34.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 0m 34s\n",
            "\n",
            " Epoch [9/20], Train Loss: 0.4255, Validation Loss: 0.5094, Validation Accuracy: 0.7891, Validation F1: 0.7828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 10/20 [05:48<05:47, 34.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 0m 34s\n",
            "\n",
            " Epoch [10/20], Train Loss: 0.4107, Validation Loss: 0.5108, Validation Accuracy: 0.7844, Validation F1: 0.7774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  55%|█████▌    | 11/20 [06:23<05:12, 34.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11 | Time: 0m 34s\n",
            "\n",
            " Epoch [11/20], Train Loss: 0.3977, Validation Loss: 0.5058, Validation Accuracy: 0.7910, Validation F1: 0.7848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 12/20 [06:58<04:39, 34.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12 | Time: 0m 35s\n",
            "\n",
            " Epoch [12/20], Train Loss: 0.3860, Validation Loss: 0.4991, Validation Accuracy: 0.7984, Validation F1: 0.7935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  65%|██████▌   | 13/20 [07:33<04:03, 34.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Time: 0m 34s\n",
            "\n",
            " Epoch [13/20], Train Loss: 0.3766, Validation Loss: 0.4941, Validation Accuracy: 0.8038, Validation F1: 0.7995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|███████   | 14/20 [08:07<03:28, 34.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 14 | Time: 0m 34s\n",
            "\n",
            " Epoch [14/20], Train Loss: 0.3687, Validation Loss: 0.4896, Validation Accuracy: 0.8073, Validation F1: 0.8037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 15/20 [08:43<02:54, 34.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 15 | Time: 0m 35s\n",
            "\n",
            " Epoch [15/20], Train Loss: 0.3604, Validation Loss: 0.4922, Validation Accuracy: 0.8049, Validation F1: 0.8009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 16/20 [09:17<02:19, 34.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 16 | Time: 0m 34s\n",
            "\n",
            " Epoch [16/20], Train Loss: 0.3542, Validation Loss: 0.4877, Validation Accuracy: 0.8105, Validation F1: 0.8072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  85%|████████▌ | 17/20 [09:52<01:44, 34.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 17 | Time: 0m 34s\n",
            "\n",
            " Epoch [17/20], Train Loss: 0.3497, Validation Loss: 0.4854, Validation Accuracy: 0.8125, Validation F1: 0.8095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%|█████████ | 18/20 [10:26<01:09, 34.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 18 | Time: 0m 34s\n",
            "\n",
            " Epoch [18/20], Train Loss: 0.3446, Validation Loss: 0.4850, Validation Accuracy: 0.8123, Validation F1: 0.8093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  95%|█████████▌| 19/20 [11:02<00:34, 34.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 19 | Time: 0m 35s\n",
            "\n",
            " Epoch [19/20], Train Loss: 0.3409, Validation Loss: 0.4890, Validation Accuracy: 0.8077, Validation F1: 0.8041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 20/20 [11:36<00:00, 34.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 20 | Time: 0m 34s\n",
            "\n",
            " Epoch [20/20], Train Loss: 0.3383, Validation Loss: 0.4885, Validation Accuracy: 0.8085, Validation F1: 0.8051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WapTzwTGj1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwLGWmQFDwrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ4shD5mDwrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF-nfyBeDwrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoJQqN2CDwrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}