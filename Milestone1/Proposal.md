# Topic : Classifying human written text from GPT-2 generated text

## Introduction:

- Task Description:  Binary classification to differentiate between human generated text and GPT-2 generated text.
- Human generated text consists of paragraphs from the WebText dataset, which is a collection of text from blogs, papers and code curated by OpenAI.
- GPT-2 generated text are the outputs from the GPT-2 model, trained on WebText.
- The scope of the project is to perform binary classification using neural network models and provide an error analysis.

##  Motivation and Contributions/Originality:

- OpenAI ‘s GPT-2 text generating model was released in 2019 and it immediately dominated the headlines. The language model has the capacity to produce an entire short story when fed with a seed like “Once upon a time”. The generated story is an amazing improvement over GPT-2’s contemporaries, other text generation algorithms.
- OpenAI initially created four language models with varying hyperparameters (i.e., 117M, 345M, 762M and 1542M).  Due to concerns of misuse like fake news generation, these models were released in stages from smallest to largest over the span of 10 months.
- The ability to analyze and detect fake news will help combat the threat of “weaponizing language models” (source). 
- The motivation of this project is to contribute to the field of fake news detection by building a classification model that can identify synthetically generated text. 

## Data:

- The human generated text is 250K documents from the WebText dataset.
- The GPT-2  generated text includes two 250K documents, labeled Temperature-1 and Top-k40. These terms are hyperparameters of the GPT-2 model that affect the randomness in output.  
- We will be analyzing the Top-k40 dataset, starting with 500 documents. We plan to scale to the maximum possible training size, as permitted by the computing resources at hand.
- The language in both datasets is restricted to English.

## Engineering:

- We will be using the GPU provided by Google Colab.
- We plan on training a CNN and BERT, and if feasible, FastText as well.
- We will be using the Pytorch framework for model implementation, including TorchText for text preprocessing.


## Previous Work:
- [“Language Models are Unsupervised Multitask Learners”](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2018)
   - At the time of publication, machine learning approaches predominantly involved training models on a single task with a carefully curated dataset. The authors strayed from the traditional and aimed to train a multitask language model through zero-shot learning with an immense amount of data collected from across domains. 
   - This type of unsupervised learning enabled the model to perform many tasks, such as reading comprehension, translation and Q&A. The research produced four language models, the largest of which is named GPT-2. Even with no fine-tuning, baseline GPT-2 performed at state-of-the-art level in 7 out of 8 language related tasks. 
- [“Release Strategies and the Social Impacts of Language Models”](https://arxiv.org/pdf/1908.09203.pdf) (2019)
   - This report explained that releasing the four variants of GPT-2 in stages was out of concern for misusing the technology. After detailing the potential for weaponizing GPT-2, the authors gave an overview of the field of text detection. They found that shorter length text is harder to predict.        -They also concluded that discriminative models like RoBERTa were more accurate in text detection than generative models like BERT. However, accuracy has yet to reach 100% so there was room for improvement. 
- ["Deep Faking" Political Twitter using Transfer learning and GPT-2](http://cs229.stanford.edu/proj2019aut/data/assignment_308832_raw/26647402.pdf) (2019)
   - Researchers trained GPT-2 on Donald Trump’s tweets, and then used another politician’s tweets as input to the model to successfully generate posts that mimicked Trump’s style, demonstrating transfer learning. 
   - A Keras 2-layer LSTM was trained in a similar manner to act as the baseline. The output from the two models clearly show the superiority and sophistication of GPT-2. 

## Evaluation:

- We will use a classification matrix for model evaluation and report the precision, recall and F1 score (micro & macro).
- Since we plan on training at least two language models, we can also compare and contrast model performances.
- In addition to evaluation, we will try to perform an error analysis and determine which features led to a model’s prediction. 
- If we have time and result in using both Top-k40 and Temperature-1 datasets, the task will become multiclass classification instead. In this case, we can compare how the models differentiate between the sentences generated by different GPT-2 configurations.


## Conclusion:
- We hope to deliver two classification models, CNN and BERT, and provide a detailed model and error analysis that highlight any insights and challenges faced in the task of classifying human text from GPT-2 text.



