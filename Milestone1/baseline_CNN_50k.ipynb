{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Baseline_CNN_50k.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvEg-06BHycP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6faa35ef-a483-4911-ab3c-0b34d65ef95f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fGixuwMDwpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.data import Iterator, BucketIterator\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/data/\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5z9RPU7P4sS",
        "colab_type": "code",
        "outputId": "50c47818-5252-4b8d-cf7f-1b3d74dc32df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -U torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us_0vewlDzgd",
        "colab_type": "code",
        "outputId": "f129845f-2c1a-4ee4-f92d-b528f64f216e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYh0BzRTDwpu",
        "colab_type": "code",
        "outputId": "2574819c-d948-424f-f606-6eb24effbd60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "manual_seed = 77\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed(manual_seed)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHMsTWXgDwp5",
        "colab_type": "text"
      },
      "source": [
        "#EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axp-pCd3Dwp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading in 50k data\n",
        "df = pd.read_csv(path+\"subset_50k.csv\", index_col=0, encoding=\"utf-8\").reset_index(drop=True)\n",
        "# df_shuffled = df.sample(frac=1, random_state=123) #shuffle rows randomly\n",
        "# df_shuffled = df_shuffled.drop(columns=\"index\") #drops index to only keep text and label\n",
        "train, validate, test = np.split(df.sample(frac=1, random_state=123).drop(columns=\"index\"), \n",
        "                                                                          [int(.6*len(df)), int(.8*len(df))])\n",
        "train_texts, train_labels = zip(*train.values) #resulting type is tuples\n",
        "valid_texts, valid_labels = zip(*validate.values)\n",
        "test_texts, test_labels = zip(*test.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2APQb4cd6t-z",
        "colab_type": "code",
        "outputId": "f5d61cb9-632f-4379-8fac-17a44399d05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#check distribution of shuffled data by class (random state 123)\n",
        "\n",
        "print(Counter(train_labels))\n",
        "print(Counter(valid_labels))\n",
        "print(Counter(test_labels))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({1: 15119, 0: 14881})\n",
            "Counter({0: 5082, 1: 4918})\n",
            "Counter({0: 5037, 1: 4963})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug6_viDP6t4u",
        "colab_type": "code",
        "outputId": "8b0c41d1-0446-4ba5-b4e7-be6f52da133c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(train[:3])\n",
        "print(train[-3:])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text  labels\n",
            "11872  Pos Gamer Achievements Won TA GS Date Finished...       0\n",
            "40828  (Note: This piece reflects the experiences of ...       1\n",
            "36400  It's been five years since my blog, which is s...       1\n",
            "                                                    text  labels\n",
            "39368  1 Clean TSP Ep. 125: \"Sleeping Beauty\" Part 2 ...       1\n",
            "47764  You have found a web site that can help you ac...       1\n",
            "8386   Andrew reviews the silly and weird horror movi...       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jscE4_hGNba",
        "colab_type": "code",
        "outputId": "4053af30-0516-4df6-cb09-bb5521b29aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(validate[:3])\n",
        "print(validate[-3:])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text  labels\n",
            "1929   The first thing you perceive when the airbag g...       0\n",
            "20865  State officials hatched a plan to bring 1,100 ...       0\n",
            "1385   Formula E could be set to welcome as many as s...       0\n",
            "                                                    text  labels\n",
            "16140  One of the best ways to earn more as a freelan...       0\n",
            "16809  President Barack Obama on Wednesday night vowe...       0\n",
            "23772  NEW YORK, NY - APRIL 13: NBA Commissioner Adam...       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxev-MELGNYX",
        "colab_type": "code",
        "outputId": "66f463b1-a56f-4c08-c724-92bc0da95bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(test[:3])\n",
        "print(test[-3:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text  labels\n",
            "1554   What percentage of illegal immigrants trying t...       0\n",
            "33704  Mitt Romney is using his time before the big d...       1\n",
            "23967  The mother of a young woman killed in a drunk-...       0\n",
            "                                                    text  labels\n",
            "17730  Just checked into the speaker forum to look fo...       0\n",
            "28030  Rape victims in Scotland have expressed frustr...       1\n",
            "15725  Touch Of Malice Check List\\n\\nMausoleum\\n\\n1\\n...       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcIpj5u_GNVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PROuds5qDwqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create train, val, test subsets from 50k subset\n",
        "\n",
        "# train.to_csv(path+\"subset_50k_train.csv\", index=False)\n",
        "# validate.to_csv(path+\"subset_50k_valid.csv\", index=False)\n",
        "# test.to_csv(path+\"subset_50k_test.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WwCrAhh-DwqT",
        "colab_type": "code",
        "outputId": "599c6197-3833-47b2-872e-25f93e81aaf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(len(train))\n",
        "print(len(validate))\n",
        "print(len(test))\n",
        "\n",
        "print(len(train_texts))\n",
        "print(len(valid_texts))\n",
        "print(len(test_texts))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n",
            "10000\n",
            "10000\n",
            "30000\n",
            "10000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGVuozlj-3ga",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v0ZfjMb-3MX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HXiqTnUDwqY",
        "colab_type": "code",
        "outputId": "c2827b1f-9c85-4b22-8450-a7245e64dd32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#50k subset\n",
        "\n",
        "n_jobs=None\n",
        "verbose=False\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "vect = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_features=2**21)\n",
        "train_features = vect.fit_transform(train_texts)\n",
        "valid_features = vect.transform(valid_texts)\n",
        "test_features = vect.transform(test_texts)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "params = {'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100]} #changed from original code\n",
        "search = GridSearchCV(model, params, cv=5, n_jobs=n_jobs, verbose=verbose)\n",
        "search.fit(sparse.vstack([train_features, valid_features]), train_labels+valid_labels)\n",
        "print(search.best_params_)\n",
        "model = model.set_params(**search.best_params_)\n",
        "model.fit(train_features, train_labels)\n",
        "valid_accuracy = model.score(valid_features, valid_labels)*100.\n",
        "test_accuracy = model.score(test_features, test_labels)*100.\n",
        "data = {\n",
        "    'valid_accuracy':valid_accuracy,\n",
        "    'test_accuracy':test_accuracy\n",
        "}\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"time in seconds:\", end-start)\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 100}\n",
            "{'valid_accuracy': 85.79, 'test_accuracy': 87.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7niM3RYDwqi",
        "colab_type": "text"
      },
      "source": [
        "# Load and prepare dataset for pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPyePrH3Dwqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spacy_en = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\",\"textcat\"])\n",
        "# spacy_en.add_pipe(spacy_en.create_pipe('sentencizer')) #adding sentence tokenizer\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm') #changed syntax?\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
        "LABEL = Field(sequential=False, unk_token = None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb-Pc1vtDwqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#50k subset\n",
        "train, val, test = TabularDataset.splits(\n",
        "               path=path, # the root directory where the data lies\n",
        "               train='subset_50k_train.csv', validation=\"subset_50k_valid.csv\", test=\"subset_50k_test.csv\", # file names\n",
        "               format='csv',\n",
        "               skip_header=True, \n",
        "               fields=[('text', TEXT), ('label', LABEL)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBZUYNmrDwqu",
        "colab_type": "code",
        "outputId": "ac4b90d9-7691-4b88-8dbc-bf38dfffa09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#50k subset\n",
        "\n",
        "TEXT.build_vocab(train, min_freq=2)\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n",
        "print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size of TEXT: 99620\n",
            "Vocabulary size of LABEL: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "555zXk3JAMyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOZ4YW9KAMvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnPFViAHAMsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45ZqiRdsDwqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, val_iter, test_iter = BucketIterator.splits( #don't we usually not do test_iter?????\n",
        " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
        " batch_sizes=(64,256,256),\n",
        " sort_key=lambda x: len(x.text), \n",
        " sort=True,\n",
        "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
        " sort_within_batch=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWGVVU7dI0UD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLzV6PFZI0Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZNAD9ecDwq5",
        "colab_type": "code",
        "outputId": "802ca581-8586-4bbc-a19a-0cf2499a02d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# create a single batch and terminate the loop\n",
        "i = 0\n",
        "for batch in val_iter:\n",
        "    texts = batch.text\n",
        "    labels = batch.label\n",
        "    print(i)\n",
        "    print(texts)\n",
        "    print(labels)\n",
        "    i+=1\n",
        "    if i ==3:\n",
        "      break\n",
        "\n",
        "    #break  #we use first batch as an example.\n",
        "    \n",
        "print('texts:', texts.shape)\n",
        "print('labels:', labels.shape)\n",
        "\n",
        "# texts: [length, batch size]\n",
        "# labels: [batch size]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "tensor([[ 4209,    11,    19,  ...,     0,     0,  8234],\n",
            "        [   17,    48,   175,  ...,     0,     8,  6759],\n",
            "        [ 4058,    21,    74,  ...,  5594, 17987,  2189],\n",
            "        ...,\n",
            "        [  104,  1347,    28,  ...,     1,     1,     1],\n",
            "        [   30,     4, 17757,  ...,     1,     1,     1],\n",
            "        [    0,    11,     4,  ...,     1,     1,     1]])\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0])\n",
            "1\n",
            "tensor([[   21,  8789,    85,  ...,     2,    14,   389],\n",
            "        [   26,   512,     7,  ...,   280,    13, 31864],\n",
            "        [   28,     3,   643,  ...,   467,   639,  1317],\n",
            "        ...,\n",
            "        [    7,   124,  1920,  ...,     1,     1,     1],\n",
            "        [  234,     5,  4226,  ...,     1,     1,     1],\n",
            "        [  660,   341,     4,  ...,     1,     1,     1]])\n",
            "tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1])\n",
            "2\n",
            "tensor([[ 931,  201, 2082,  ...,    7, 6177,  253],\n",
            "        [  39,  951,    5,  ..., 1992, 1948,    7],\n",
            "        [ 443,  171,    0,  ..., 6734,  197,  945],\n",
            "        ...,\n",
            "        [   4,    0,  166,  ...,    1,    1,    1],\n",
            "        [   8,   31, 1758,  ...,    1,    1,    1],\n",
            "        [ 341,    4,    4,  ...,    1,    1,    1]])\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
            "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1])\n",
            "texts: torch.Size([94, 256])\n",
            "labels: torch.Size([256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru0oxXl1Dwq-",
        "colab_type": "text"
      },
      "source": [
        "# CNN baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRB5tHL1Dwq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To define a CNN class\n",
        "class CNN_Text(nn.Module):\n",
        "    def __init__(self, vocabulary_size, embedding_dim, output_size, kernel_num, region_sizes, dropout):\n",
        "        '''\n",
        "        vocabulary_size: vocabulary size\n",
        "        embedding_dim: word embedding size\n",
        "        output_size: number of classes in prediction\n",
        "        kernel_num: number of kernels (number of output channels of convolutional layers)\n",
        "        region_sizes: height of kernels of convolutional layers\n",
        "        dropout: dropout rate\n",
        "        '''\n",
        "        super(CNN_Text, self).__init__()\n",
        "        # the size of input channel is 1.\n",
        "        Ci = 1\n",
        "        \n",
        "        # word embedding layer\n",
        "        self.embeddings = nn.Embedding(num_embeddings = vocabulary_size, embedding_dim = embedding_dim )\n",
        "        \n",
        "        # convolution with kernels\n",
        "        self.convolution_layers = nn.ModuleList([nn.Conv2d(in_channels = Ci, out_channels = kernel_num, kernel_size = (K, embedding_dim)) for K in region_sizes])\n",
        "        \n",
        "        # a dropout layer\n",
        "        self.dropout = nn.Dropout(dropout) \n",
        "        \n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * kernel_num, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input x  [sequence length, batch size]\n",
        "        \n",
        "        input_embeddings = self.embeddings(x)  \n",
        "        # (batch size, word_sequence, embedding_dim) word embedding\n",
        "\n",
        "        input_embeddings = input_embeddings.permute(1,0,2)\n",
        "        input_embeddings = input_embeddings.unsqueeze(1)\n",
        "        #  [batch size, number of channel is one, sequence length, embeeding size]\n",
        "\n",
        "        # convolutional layers\n",
        "        convolute_outputs = [F.relu(conv(input_embeddings)).squeeze(3) for conv in self.convolution_layers]  \n",
        "        \n",
        "        # to get the maximum value of filtered tensor\n",
        "        max_pooling_outputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in convolute_outputs] \n",
        "        \n",
        "        concat_list = torch.cat(max_pooling_outputs, 1) # concatenate representations\n",
        "        \n",
        "        drop_output = self.dropout(concat_list)  # add drop layer\n",
        "        \n",
        "        fc1_output = self.fc(drop_output)  # get the fc1 using a fully connected layer\n",
        "        \n",
        "        final_output = F.softmax(fc1_output,dim=1)\n",
        "        \n",
        "        return final_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl95C6aODwrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters\n",
        "\n",
        "# the vocabulary size\n",
        "vocabulary_size = len(TEXT.vocab.stoi) \n",
        "\n",
        "# Dimension of word embedding is 300. Namely, each word is expressed by a vector that has 300 dimensions.\n",
        "embedding_dim = 300 \n",
        "\n",
        "# region size as 2, 3, and 4\n",
        "kernel_sizes = [2,3,4] \n",
        "\n",
        "# the number of kernel in each region size\n",
        "kernels_num = 32  \n",
        "\n",
        "# The dropout rate is set to be 0.5.\n",
        "dropout = 0.5\n",
        "\n",
        "# The output size of labels.\n",
        "output_size = 2\n",
        "\n",
        "# learning rate is set to be 0.01. CHANGED FROM ORIGINAL\n",
        "lr = 0.0001        \n",
        "\n",
        "# The number of iteration is set to be 5.\n",
        "num_epoch = 5  \n",
        "\n",
        "# employ class CNN_Text and assign to cnn\n",
        "model = CNN_Text(vocabulary_size, embedding_dim, output_size, kernels_num, kernel_sizes, dropout).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZGhseSCDwrI",
        "colab_type": "code",
        "outputId": "bf407745-5ee5-4445-d5e4-9a52e2ab59a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_Text(\n",
              "  (embeddings): Embedding(100564, 300)\n",
              "  (convolution_layers): ModuleList(\n",
              "    (0): Conv2d(1, 32, kernel_size=(2, 300), stride=(1, 1))\n",
              "    (1): Conv2d(1, 32, kernel_size=(3, 300), stride=(1, 1))\n",
              "    (2): Conv2d(1, 32, kernel_size=(4, 300), stride=(1, 1))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=96, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3nnVOT7DwrN",
        "colab_type": "code",
        "outputId": "6e86e948-f820-473d-c326-c27671c12e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 30,255,890 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cccAFOyUDwrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)   # define a optimizer for backpropagation\n",
        "loss_func = nn.CrossEntropyLoss()   # define loss funtion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxVXTzNCDwrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        batch_input, labels = batch.text, batch.label\n",
        "        batch_input = batch_input.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(batch_input)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.cpu().item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            batch_input, labels = batch.text, batch.label\n",
        "            batch_input = batch_input.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_input)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            epoch_loss += loss.cpu().item()\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return epoch_loss / len(iterator), accuracy, f1score\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tnMpUT6D5VN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB38dh_wD5R8",
        "colab_type": "code",
        "outputId": "d04fd722-4533-44b5-d57b-9e725aec8482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(val_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXWX9rwLDwra",
        "colab_type": "code",
        "outputId": "0edf5060-67f4-40b8-ed0b-2f1342c971f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 50k subset\n",
        "MAX_EPOCH = 20\n",
        "total_step = len(train_iter)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(MAX_EPOCH, desc=\"Epoch\"):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iter, optimizer, loss_func)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(model, val_iter, loss_func)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    # state_dict_model = model.state_dict() \n",
        "    # state = {\n",
        "    #     'epoch': epoch,\n",
        "    #     'state_dict': state_dict_model,\n",
        "    #     'optimizer': optimizer.state_dict()\n",
        "    #     }\n",
        "\n",
        "    #torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt_cnn/CNN_TEXT_\"+str(epoch+1)+\".pt\")\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, \n",
        "                                                                MAX_EPOCH, train_loss, val_loss, val_acc, val_f1))\n",
        "    "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   5%|▌         | 1/20 [00:15<04:52, 15.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 15s\n",
            "\n",
            " Epoch [1/20], Train Loss: 0.5785, Validation Loss: 0.5813, Validation Accuracy: 0.7023, Validation F1: 0.6966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 2/20 [00:30<04:36, 15.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 0m 15s\n",
            "\n",
            " Epoch [2/20], Train Loss: 0.5694, Validation Loss: 0.5742, Validation Accuracy: 0.7163, Validation F1: 0.7122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  15%|█▌        | 3/20 [00:46<04:21, 15.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 0m 15s\n",
            "\n",
            " Epoch [3/20], Train Loss: 0.5582, Validation Loss: 0.5671, Validation Accuracy: 0.7285, Validation F1: 0.7257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 4/20 [01:01<04:06, 15.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 0m 15s\n",
            "\n",
            " Epoch [4/20], Train Loss: 0.5496, Validation Loss: 0.5623, Validation Accuracy: 0.7339, Validation F1: 0.7313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 5/20 [01:17<03:53, 15.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 0m 15s\n",
            "\n",
            " Epoch [5/20], Train Loss: 0.5385, Validation Loss: 0.5512, Validation Accuracy: 0.7520, Validation F1: 0.7513\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 6/20 [01:32<03:36, 15.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 0m 15s\n",
            "\n",
            " Epoch [6/20], Train Loss: 0.5307, Validation Loss: 0.5456, Validation Accuracy: 0.7594, Validation F1: 0.7589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  35%|███▌      | 7/20 [01:48<03:20, 15.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 0m 15s\n",
            "\n",
            " Epoch [7/20], Train Loss: 0.5200, Validation Loss: 0.5417, Validation Accuracy: 0.7633, Validation F1: 0.7627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 8/20 [02:03<03:04, 15.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 0m 15s\n",
            "\n",
            " Epoch [8/20], Train Loss: 0.5108, Validation Loss: 0.5367, Validation Accuracy: 0.7702, Validation F1: 0.7697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  45%|████▌     | 9/20 [02:18<02:49, 15.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 0m 15s\n",
            "\n",
            " Epoch [9/20], Train Loss: 0.4982, Validation Loss: 0.5316, Validation Accuracy: 0.7757, Validation F1: 0.7753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 10/20 [02:34<02:35, 15.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 0m 15s\n",
            "\n",
            " Epoch [10/20], Train Loss: 0.4869, Validation Loss: 0.5268, Validation Accuracy: 0.7830, Validation F1: 0.7827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  55%|█████▌    | 11/20 [02:49<02:18, 15.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11 | Time: 0m 15s\n",
            "\n",
            " Epoch [11/20], Train Loss: 0.4763, Validation Loss: 0.5246, Validation Accuracy: 0.7860, Validation F1: 0.7855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 12/20 [03:05<02:03, 15.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12 | Time: 0m 15s\n",
            "\n",
            " Epoch [12/20], Train Loss: 0.4637, Validation Loss: 0.5187, Validation Accuracy: 0.7924, Validation F1: 0.7922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  65%|██████▌   | 13/20 [03:20<01:47, 15.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Time: 0m 15s\n",
            "\n",
            " Epoch [13/20], Train Loss: 0.4519, Validation Loss: 0.5148, Validation Accuracy: 0.7960, Validation F1: 0.7959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|███████   | 14/20 [03:35<01:32, 15.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 14 | Time: 0m 15s\n",
            "\n",
            " Epoch [14/20], Train Loss: 0.4411, Validation Loss: 0.5121, Validation Accuracy: 0.7992, Validation F1: 0.7990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 15/20 [03:50<01:16, 15.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 15 | Time: 0m 15s\n",
            "\n",
            " Epoch [15/20], Train Loss: 0.4284, Validation Loss: 0.5088, Validation Accuracy: 0.8002, Validation F1: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 16/20 [04:06<01:01, 15.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 16 | Time: 0m 15s\n",
            "\n",
            " Epoch [16/20], Train Loss: 0.4189, Validation Loss: 0.5060, Validation Accuracy: 0.8022, Validation F1: 0.8021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  85%|████████▌ | 17/20 [04:22<00:46, 15.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 17 | Time: 0m 15s\n",
            "\n",
            " Epoch [17/20], Train Loss: 0.4072, Validation Loss: 0.5033, Validation Accuracy: 0.8036, Validation F1: 0.8035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%|█████████ | 18/20 [04:37<00:30, 15.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 18 | Time: 0m 15s\n",
            "\n",
            " Epoch [18/20], Train Loss: 0.3985, Validation Loss: 0.5018, Validation Accuracy: 0.8037, Validation F1: 0.8036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  95%|█████████▌| 19/20 [04:52<00:15, 15.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 19 | Time: 0m 15s\n",
            "\n",
            " Epoch [19/20], Train Loss: 0.3882, Validation Loss: 0.5004, Validation Accuracy: 0.8037, Validation F1: 0.8036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 20/20 [05:08<00:00, 15.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 20 | Time: 0m 15s\n",
            "\n",
            " Epoch [20/20], Train Loss: 0.3811, Validation Loss: 0.4989, Validation Accuracy: 0.8067, Validation F1: 0.8066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WapTzwTGj1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNsYQVOaDwrg",
        "colab_type": "code",
        "outputId": "0a028232-2ee1-4ee3-b5be-8fe332a65d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 100k subset\n",
        "\n",
        "MAX_EPOCH = 20\n",
        "total_step = len(train_iter)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(MAX_EPOCH, desc=\"Epoch\"):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iter, optimizer, loss_func)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(model, val_iter, loss_func)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    # state_dict_model = model.state_dict() \n",
        "    # state = {\n",
        "    #     'epoch': epoch,\n",
        "    #     'state_dict': state_dict_model,\n",
        "    #     'optimizer': optimizer.state_dict()\n",
        "    #     }\n",
        "\n",
        "    #torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt_cnn/CNN_TEXT_\"+str(epoch+1)+\".pt\")\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, \n",
        "                                                                MAX_EPOCH, train_loss, val_loss, val_acc, val_f1))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   5%|▌         | 1/20 [02:09<41:00, 129.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 9s\n",
            "\n",
            " Epoch [1/20], Train Loss: 0.8533, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 2/20 [04:19<38:51, 129.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 2m 9s\n",
            "\n",
            " Epoch [2/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  15%|█▌        | 3/20 [06:30<36:51, 130.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 2m 11s\n",
            "\n",
            " Epoch [3/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 4/20 [08:41<34:44, 130.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 2m 10s\n",
            "\n",
            " Epoch [4/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 5/20 [10:51<32:32, 130.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 2m 9s\n",
            "\n",
            " Epoch [5/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 6/20 [13:00<30:19, 129.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 2m 9s\n",
            "\n",
            " Epoch [6/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  35%|███▌      | 7/20 [15:11<28:12, 130.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 2m 10s\n",
            "\n",
            " Epoch [7/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 8/20 [17:20<25:59, 129.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 2m 9s\n",
            "\n",
            " Epoch [8/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  45%|████▌     | 9/20 [19:30<23:48, 129.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 2m 9s\n",
            "\n",
            " Epoch [9/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 10/20 [21:39<21:37, 129.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 2m 9s\n",
            "\n",
            " Epoch [10/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  55%|█████▌    | 11/20 [23:50<19:30, 130.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11 | Time: 2m 10s\n",
            "\n",
            " Epoch [11/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 12/20 [26:00<17:18, 129.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12 | Time: 2m 9s\n",
            "\n",
            " Epoch [12/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  65%|██████▌   | 13/20 [28:09<15:08, 129.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Time: 2m 9s\n",
            "\n",
            " Epoch [13/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|███████   | 14/20 [30:18<12:57, 129.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 14 | Time: 2m 9s\n",
            "\n",
            " Epoch [14/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 15/20 [32:29<10:49, 129.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 15 | Time: 2m 10s\n",
            "\n",
            " Epoch [15/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 16/20 [34:38<08:38, 129.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 16 | Time: 2m 9s\n",
            "\n",
            " Epoch [16/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  85%|████████▌ | 17/20 [36:48<06:28, 129.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 17 | Time: 2m 9s\n",
            "\n",
            " Epoch [17/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%|█████████ | 18/20 [38:57<04:19, 129.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 18 | Time: 2m 9s\n",
            "\n",
            " Epoch [18/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  95%|█████████▌| 19/20 [41:07<02:09, 129.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 19 | Time: 2m 10s\n",
            "\n",
            " Epoch [19/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 20/20 [43:17<00:00, 129.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 20 | Time: 2m 9s\n",
            "\n",
            " Epoch [20/20], Train Loss: 0.8113, Validation Loss: 0.8195, Validation Accuracy: 0.4938, Validation F1: 0.3306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwLGWmQFDwrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ4shD5mDwrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF-nfyBeDwrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoJQqN2CDwrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}