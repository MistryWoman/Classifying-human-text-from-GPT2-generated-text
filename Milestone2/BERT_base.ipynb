{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_base.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e50cb49bc6b64052b619b8f31d2ba38d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4c380e727b784c90be4af406d24fc334",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba1f42fa3a334b928420fbdcb34976c5",
              "IPY_MODEL_a069221b81d44dfc8c4e01af4dd47af8"
            ]
          }
        },
        "4c380e727b784c90be4af406d24fc334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba1f42fa3a334b928420fbdcb34976c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f0169293dfe24824a37b672b2762e467",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d9a651de53e45ed95ca87b6f203fe54"
          }
        },
        "a069221b81d44dfc8c4e01af4dd47af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1641df5ddb3d4d18a605facde6925ef2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 361/361 [15:29&lt;00:00, 2.57s/B]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_55f9ac2bf95e4edeb0ad4317c80db308"
          }
        },
        "f0169293dfe24824a37b672b2762e467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d9a651de53e45ed95ca87b6f203fe54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1641df5ddb3d4d18a605facde6925ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "55f9ac2bf95e4edeb0ad4317c80db308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "513bfb720d4b4cad89c462382bc03af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_97d658370c394a91ab791789941a04c6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d0ba280905548f88c96608f443539f0",
              "IPY_MODEL_d6d8d2560df34c8aba8d16e14e22b5dc"
            ]
          }
        },
        "97d658370c394a91ab791789941a04c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d0ba280905548f88c96608f443539f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5401aab0fdff43ca93f2199dd84af1cb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92276010a6304c9e961d2bfa722d8a59"
          }
        },
        "d6d8d2560df34c8aba8d16e14e22b5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7557f6662ae342d9aea512558b009132",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:38&lt;00:00, 11.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e711d6d631e4284a9e035afbfe96380"
          }
        },
        "5401aab0fdff43ca93f2199dd84af1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92276010a6304c9e961d2bfa722d8a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7557f6662ae342d9aea512558b009132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e711d6d631e4284a9e035afbfe96380": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr5RQxrT8eaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s5La4ay8t9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "430d6455-229e-43e4-848f-0d206c98719e"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.data import Iterator, BucketIterator\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "import time\n",
        "from collections import Counter\n",
        "import tensorflow\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/585_group/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okj_5VwV8xuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "164014d6-dcf2-4000-b201-7b926a0dbc5f"
      },
      "source": [
        "## Set seed of randomization and working device\n",
        "manual_seed = 77\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed(manual_seed)\n",
        "\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CmNQ2qL82xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading in 100k data\n",
        "df = pd.read_csv(path+\"subset_100k.csv\", index_col=0, encoding=\"utf-8\").reset_index(drop=True)\n",
        "# df_shuffled = df.sample(frac=1, random_state=123) #shuffle rows randomly\n",
        "# df_shuffled = df_shuffled.drop(columns=\"index\") #drops index to only keep text and label\n",
        "train, validate, test = np.split(df.sample(frac=1, random_state=123).drop(columns=\"index\"), \n",
        "                                                                          [int(.6*len(df)), int(.8*len(df))])\n",
        "train_texts, train_labels = zip(*train.values) #resulting type is tuples\n",
        "valid_texts, valid_labels = zip(*validate.values)\n",
        "test_texts, test_labels = zip(*test.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89bSu0NL9R2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6ee408d8-73e9-46ed-ed7f-26ee1cb914e0"
      },
      "source": [
        "#check distribution of shuffled data by class (random state 123)\n",
        "\n",
        "print(Counter(train_labels))\n",
        "print(Counter(valid_labels))\n",
        "print(Counter(test_labels))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0: 30118, 1: 29882})\n",
            "Counter({1: 10124, 0: 9876})\n",
            "Counter({0: 10006, 1: 9994})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNOAA41E9XX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "969836ff-ac92-4cf2-be28-48c521ab6d52"
      },
      "source": [
        "print(len(train))\n",
        "print(len(validate))\n",
        "print(len(test))\n",
        "\n",
        "print(len(train_texts))\n",
        "print(len(valid_texts))\n",
        "print(len(test_texts))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "20000\n",
            "20000\n",
            "60000\n",
            "20000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yoB4lOj9cRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "edeb58a0-4db9-4bda-f55a-dd13e2682758"
      },
      "source": [
        "# installing transformers since BERT resembles transformer encoder\n",
        "! pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 1.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 17.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.35)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.35 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.35)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=d1c15b4f9a1ca1f0c209207184a5ba33c2cc171ab63be436ae770712e525f822\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-fEvrNz9r40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5vrzg0R9vZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function for data preparation\n",
        "def data_prepare(file_path, lab2ind, tokenizer, max_len = 32, mode = 'train'):\n",
        "    '''\n",
        "    file_path: the path to input file. \n",
        "                In train mode, the input must be a tsv file that includes two columns where the first is text, and second column is label.\n",
        "                The first row must be header of columns.\n",
        "\n",
        "                In predict mode, the input must be a tsv file that includes only one column where the first is text.\n",
        "                The first row must be header of column.\n",
        "\n",
        "    lab2ind: dictionary of label classes\n",
        "    tokenizer: BERT tokenizer\n",
        "    max_len: maximal length of input sequence\n",
        "    mode: train or predict\n",
        "    '''\n",
        "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
        "    if mode == 'train':\n",
        "        # Use pandas to load dataset\n",
        "        df = pd.read_csv(file_path, delimiter=',',header=0, names=['content','label'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        labels = df.label.values\n",
        "        \n",
        "        # Create sentence and label lists\n",
        "        labels = [i for i in labels] \n",
        "        print(\"Label is \", labels[0])\n",
        "        \n",
        "        # Convert data into torch tensors\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # if we are in predict mode, we will load one column (i.e., text).\n",
        "    elif mode == 'predict':\n",
        "        df = pd.read_csv(file_path, delimiter=',',header=0, names=['content'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        # create placeholder\n",
        "        labels = []\n",
        "    else:\n",
        "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
        "        return\n",
        "        \n",
        "    # Create sentence and label lists\n",
        "    content = df.content.values\n",
        "\n",
        "    # We need to add a special token at the beginning for BERT to work properly.\n",
        "    content = [\"[CLS] \" + text for text in content]\n",
        "\n",
        "    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "    tokenized_texts = [tokenizer.tokenize(text) for text in content]\n",
        "\n",
        "    \n",
        "    # if the sequence is longer the maximal length, we truncate it to the pre-defined maximal length\n",
        "    tokenized_texts = [ text[:max_len+1] for text in tokenized_texts]\n",
        "\n",
        "    # We also need to add a special token at the end.\n",
        "    tokenized_texts = [ text+['[SEP]'] for text in tokenized_texts]\n",
        "    print (\"Tokenize the first sentence:\\n\",tokenized_texts[0])\n",
        "    \n",
        "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    print (\"Index numbers of the first sentence:\\n\",input_ids[0])\n",
        "\n",
        "    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "    print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for pad tokens\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Convert all of our data into torch tensors, the required datatype for our model\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, labels, masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jshmh6eo-T7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a binary classifier, 0 is GTP-2 generated, 1 is webtext\n",
        "# lab2ind = {'GTP-2': 0, 'webtext': 1}\n",
        "\n",
        "# tokenizer from pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRbAz8Bh-iqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "cf14597c-5d51-44d4-9635-7750389e2130"
      },
      "source": [
        "# Use defined funtion to extract data\n",
        "train_inputs, train_labels, train_masks = data_prepare(path+\"subset_100k_train.csv\", lab2ind,tokenizer)\n",
        "validation_inputs, validation_labels, validation_masks = data_prepare(path+\"subset_100k_valid.csv\", lab2ind,tokenizer)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size  (60000, 2)\n",
            "Label is  0\n",
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'mumbai', ':', 'india', \"'\", 's', 'star', 'professional', 'boxer', 'and', 'olympic', 'bronze', 'medalist', 'vi', '##je', '##nder', 'singh', 'will', 'clash', 'with', 'china', \"'\", 's', 'undefeated', ',', 'left', '-', 'handed', 'fighter', 'zu', '##lp', '##ika', '##r', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 8955, 1024, 2634, 1005, 1055, 2732, 2658, 10423, 1998, 4386, 4421, 12968, 6819, 6460, 11563, 5960, 2097, 13249, 2007, 2859, 1005, 1055, 15188, 1010, 2187, 1011, 4375, 4959, 16950, 14277, 7556, 2099, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [  101  8955  1024  2634  1005  1055  2732  2658 10423  1998  4386  4421\n",
            " 12968  6819  6460 11563  5960  2097 13249  2007  2859  1005  1055 15188\n",
            "  1010  2187  1011  4375  4959 16950 14277  7556  2099   102]\n",
            "Data size  (20000, 2)\n",
            "Label is  1\n",
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'a', 'recent', 'study', 'has', 'found', 'that', 'children', 'are', 'more', 'willing', 'to', 'take', 'risks', 'and', 'take', 'the', 'risk', 'of', 'more', 'severe', 'punishment', 'than', 'adults', '.', 'at', 'the', 'bottom', 'of', 'your', 'list', 'of', 'things', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 1037, 3522, 2817, 2038, 2179, 2008, 2336, 2024, 2062, 5627, 2000, 2202, 10831, 1998, 2202, 1996, 3891, 1997, 2062, 5729, 7750, 2084, 6001, 1012, 2012, 1996, 3953, 1997, 2115, 2862, 1997, 2477, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [  101  1037  3522  2817  2038  2179  2008  2336  2024  2062  5627  2000\n",
            "  2202 10831  1998  2202  1996  3891  1997  2062  5729  7750  2084  6001\n",
            "  1012  2012  1996  3953  1997  2115  2862  1997  2477   102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vkZs6S3-49B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "# We'll take training samples in random order in each epoch. \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, \n",
        "                              sampler = RandomSampler(train_data), # Select batches randomly\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "# We'll just read validation set sequentially.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_dataloader = DataLoader(validation_data, \n",
        "                                   sampler = SequentialSampler(validation_data), # Pull out batches sequentially.\n",
        "                                   batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nVt0CixAODo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "e50cb49bc6b64052b619b8f31d2ba38d",
            "4c380e727b784c90be4af406d24fc334",
            "ba1f42fa3a334b928420fbdcb34976c5",
            "a069221b81d44dfc8c4e01af4dd47af8",
            "f0169293dfe24824a37b672b2762e467",
            "3d9a651de53e45ed95ca87b6f203fe54",
            "1641df5ddb3d4d18a605facde6925ef2",
            "55f9ac2bf95e4edeb0ad4317c80db308",
            "513bfb720d4b4cad89c462382bc03af9",
            "97d658370c394a91ab791789941a04c6",
            "7d0ba280905548f88c96608f443539f0",
            "d6d8d2560df34c8aba8d16e14e22b5dc",
            "5401aab0fdff43ca93f2199dd84af1cb",
            "92276010a6304c9e961d2bfa722d8a59",
            "7557f6662ae342d9aea512558b009132",
            "7e711d6d631e4284a9e035afbfe96380"
          ]
        },
        "outputId": "7fe3ac09-e456-4fae-c14b-dd67725ddfc8"
      },
      "source": [
        "model_path = \"bert-base-uncased\"\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True).to(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e50cb49bc6b64052b619b8f31d2ba38d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "513bfb720d4b4cad89c462382bc03af9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9wH-iMZCiwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# explore the BERT base model\n",
        "\n",
        "dataiter = iter(train_dataloader)\n",
        "batch = dataiter.next()\n",
        "# Add batch to GPU\n",
        "batch = tuple(t.to(device) for t in batch)\n",
        "# Unpack the inputs from our dataloader\n",
        "input_ids, input_mask, labels = batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RVWV8vPCzvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_hidden_state, pooler_output, hidden_states, attentions = bert_model(input_ids, attention_mask = input_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPUm5r0sC0OV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7291e359-e863-47ba-ae9c-db0f677b977c"
      },
      "source": [
        "# sequence of hidden-states at the output of the last layer.\n",
        "last_hidden_state.shape # [batch size, seq length, hidden size]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 34, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbSDCzt3C2o3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dafc1b06-d2d1-4b96-8320-e65a623391d5"
      },
      "source": [
        "# pooler_output: last layer hidden-state of the first token of the sequence (classification token, [CLS]).\n",
        "pooler_output.shape # [batch size, hidden size]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaGbTD_hC_qV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1568f2e7-2ce4-41bc-ef9f-6cae215322c4"
      },
      "source": [
        "# (one for the output of each layer + the output of the embeddings) of shape [batch_size, sequence_length, hidden_size]: Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "print(len(hidden_states))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDEaes9MDMh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7697d5f4-69b2-4392-f341-86a16191e5c6"
      },
      "source": [
        "for i, item in enumerate(hidden_states):\n",
        "  print(\"layer \" + str(i), item.shape) # [batch size, sequence length, hidden size]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer 0 torch.Size([32, 34, 768])\n",
            "layer 1 torch.Size([32, 34, 768])\n",
            "layer 2 torch.Size([32, 34, 768])\n",
            "layer 3 torch.Size([32, 34, 768])\n",
            "layer 4 torch.Size([32, 34, 768])\n",
            "layer 5 torch.Size([32, 34, 768])\n",
            "layer 6 torch.Size([32, 34, 768])\n",
            "layer 7 torch.Size([32, 34, 768])\n",
            "layer 8 torch.Size([32, 34, 768])\n",
            "layer 9 torch.Size([32, 34, 768])\n",
            "layer 10 torch.Size([32, 34, 768])\n",
            "layer 11 torch.Size([32, 34, 768])\n",
            "layer 12 torch.Size([32, 34, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXlaUaRyDWuF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afdc4517-039c-4907-f46a-9df7e83fae52"
      },
      "source": [
        "# Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "print(len(attentions))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIVWcwmIFo29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "11f1fca7-c826-41d2-fdb6-b744b450ee07"
      },
      "source": [
        "for i, item in enumerate(attentions):\n",
        "  print(\"layer \" + str(i), item.shape) # [batch size, num_heads, sequence length, hidden size]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer 0 torch.Size([32, 12, 34, 34])\n",
            "layer 1 torch.Size([32, 12, 34, 34])\n",
            "layer 2 torch.Size([32, 12, 34, 34])\n",
            "layer 3 torch.Size([32, 12, 34, 34])\n",
            "layer 4 torch.Size([32, 12, 34, 34])\n",
            "layer 5 torch.Size([32, 12, 34, 34])\n",
            "layer 6 torch.Size([32, 12, 34, 34])\n",
            "layer 7 torch.Size([32, 12, 34, 34])\n",
            "layer 8 torch.Size([32, 12, 34, 34])\n",
            "layer 9 torch.Size([32, 12, 34, 34])\n",
            "layer 10 torch.Size([32, 12, 34, 34])\n",
            "layer 11 torch.Size([32, 12, 34, 34])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRB47_VpFx3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fc = nn.Linear(768, 2).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv7HtLpoF4wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fc_output = fc(pooler_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqLWs1EAF6yP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a057259-e87f-4a3f-dded-02671795c2f6"
      },
      "source": [
        "# We use nn.CrossEntropyLoss() as our loss function. \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(fc_output, labels)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFvLKqU8F9kW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bert_cls(nn.Module):\n",
        "    def __init__(self, lab2ind, model_path, hidden_size):\n",
        "        super(Bert_cls, self).__init__()\n",
        "        self.model_path = model_path\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bert_model = BertModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True)\n",
        "        self.label_num = len(lab2ind)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.label_num)\n",
        "    def forward(self, bert_ids, bert_mask):\n",
        "        last_hidden_state, pooler_output, hidden_states, attentions = self.bert_model(input_ids=bert_ids)\n",
        "        fc_output = self.fc(pooler_output)\n",
        "        return fc_output, attentions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CKKgJulGBke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model = Bert_cls(lab2ind, 'bert-base-uncased', 768).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VisrvdgGFl0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3c45814-05b6-47aa-f190-32de7e5c6d91"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(bert_model):,} trainable parameters')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 109,483,778 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyKxP98qGHwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters:\n",
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "epochs = 10\n",
        "warmup_proportion = 0.1\n",
        "num_training_steps  = len(train_dataloader) * epochs\n",
        "num_warmup_steps = num_training_steps * warmup_proportion\n",
        "\n",
        "### In Transformers, optimizer and schedules are instantiated like this:\n",
        "# Note: AdamW is a class from the huggingface library\n",
        "# the 'W' stands for 'Weight Decay\"\n",
        "optimizer = AdamW(bert_model.parameters(), lr=lr, correct_bias=False)\n",
        "# schedules\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\n",
        "# We use nn.CrossEntropyLoss() as our loss function. \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3oXMYMGGKXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "\n",
        "        outputs,_ = model(input_ids, input_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        # delete used variables to free GPU memory\n",
        "        del batch, input_ids, input_mask, labels\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    # free GPU memory\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHBkgcJiGNx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            # Unpack the inputs from our dataloader\n",
        "            input_ids, input_mask, labels = batch\n",
        "\n",
        "            outputs,_ = model(input_ids, input_mask)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # delete used variables to free GPU memory\n",
        "            del batch, input_ids, input_mask\n",
        "            epoch_loss += loss.cpu().item()\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return epoch_loss / len(iterator), accuracy, f1score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJLocM9rGQkg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "eb1a67cd-30bf-4a48-82fe-18875e1165b8"
      },
      "source": [
        "# Train the model\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    train_loss = train(bert_model, train_dataloader, optimizer, scheduler, criterion)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(bert_model, validation_dataloader, criterion)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': bert_model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "        }\n",
        "\n",
        "    torch.save(state, \"./drive/My Drive/Colab Notebooks/585_group/ckpt_BERT/BERT_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, epochs, train_loss, val_loss, val_acc, val_f1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch:  10%|█         | 1/10 [14:56<2:14:32, 896.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch [1/10], Train Loss: 0.0769, Validation Loss: 0.2810, Validation Accuracy: 0.9142, Validation F1: 0.9139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  20%|██        | 2/10 [29:54<1:59:36, 897.09s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch [2/10], Train Loss: 0.0750, Validation Loss: 0.4139, Validation Accuracy: 0.9087, Validation F1: 0.9082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  30%|███       | 3/10 [44:47<1:44:32, 896.03s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch [3/10], Train Loss: 0.0457, Validation Loss: 0.3915, Validation Accuracy: 0.9178, Validation F1: 0.9176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jitm5bORGTvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}